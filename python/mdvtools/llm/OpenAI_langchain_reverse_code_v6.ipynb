{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Generation using Retrieval Augmented Generation + LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import nbformat\n",
    "import os\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import datetime\n",
    "import json\n",
    "import scanpy as sc\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import langchain_experimental.agents.agent_toolkits.pandas.base as lp\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"Create a gene expression box plot for the gene CD14.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting keys and variables\n",
    "# .env file should have OPENAI_API_KEY - the .env file is no longer needed for the GITHUB token\n",
    "load_dotenv()\n",
    "\n",
    "FILE_URL_PATH = \"./\"\n",
    "FILE_URL_NAME = \"code_files_URL.txt\"\n",
    "\n",
    "mypath = os.path.abspath('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the local JSON log file\n",
    "LOG_FILE = \"./../../../chat_log.json\"\n",
    "\n",
    "# Function to ensure the JSON log file exists\n",
    "def initialize_json_log():\n",
    "    if not os.path.exists(LOG_FILE):\n",
    "        with open(LOG_FILE, 'w') as file:\n",
    "            json.dump([], file)  # Initialize with an empty list\n",
    "\n",
    "# Function to log data to the JSON file\n",
    "def log_to_json(context, prompt, prompt_template, response):\n",
    "    # Ensure the log file exists\n",
    "    initialize_json_log()\n",
    "\n",
    "    # Prepare log entry\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_entry = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"context\": context,\n",
    "        \"prompt\": prompt,\n",
    "        \"prompt_template\": prompt_template,\n",
    "        \"response\": response\n",
    "    }\n",
    "\n",
    "    # Read the existing logs\n",
    "    with open(LOG_FILE, 'r') as file:\n",
    "        logs = json.load(file)\n",
    "    \n",
    "    # Append the new log entry\n",
    "    logs.append(log_entry)\n",
    "\n",
    "    # Write back the updated logs to the file\n",
    "    with open(LOG_FILE, 'w') as file:\n",
    "        json.dump(logs, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an instance of the ChatOpenAI class with specified parameters\n",
    "# Set the temperature to 0.1 for more deterministic responses\n",
    "# Specify the model to use as \"gpt-4o\"\n",
    "\n",
    "code_llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4o\")\n",
    "\n",
    "dataframe_llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_local_repo(directory_path): #url, is_sub_dir, branch_name, project_path, access_token=f\"{GITHUB_TOKEN}\"):\n",
    "    \"\"\"\n",
    "    Crawls a local directory to retrieve file paths based on specified criteria.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path to the local project directory.\n",
    "\n",
    "    Returns:\n",
    "        list: List of file paths that match the criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of files to ignore\n",
    "    ignore_list = ['__init__.py', 'pbmc3k_tutorial.ipynb', 'pbmc3k_tutorial.py']\n",
    "\n",
    "    # Initialize an empty list to store file paths\n",
    "    files = []\n",
    "\n",
    "    # Walk through the directory tree\n",
    "    for root, dirs, file_names in os.walk(directory_path):\n",
    "        # Skip hidden directories (those starting with '.')\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        \n",
    "        for file_name in file_names:\n",
    "            # Check if the file meets the criteria for inclusion\n",
    "            if file_name not in ignore_list and (file_name.endswith('.py') or file_name.endswith('.ipynb')):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                files.append(file_path)\n",
    "\n",
    "    # Return the list of collected file paths\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl the local directory to get a list of relevant file paths\n",
    "local_directory_path = \"../test_projects/TAURUS_examples/\"\n",
    "code_files_paths = crawl_local_repo(local_directory_path)\n",
    "\n",
    "# Write the list of file paths to a specified text file\n",
    "with open(FILE_URL_PATH + FILE_URL_NAME, 'w') as f:\n",
    "    # Iterate through the list of file paths and write each one to the file\n",
    "    for item in code_files_paths:\n",
    "        f.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the Python code from a .ipynb (Jupyter Notebook) file from the local filesystem\n",
    "def extract_python_code_from_ipynb(local_file_path, cell_type=\"code\"):\n",
    "    # Read the notebook content from the local file\n",
    "    with open(local_file_path, 'r', encoding='utf-8') as f:\n",
    "        notebook_content = f.read()\n",
    "\n",
    "    # Parse the notebook content using nbformat\n",
    "    notebook = nbformat.reads(notebook_content, as_version=nbformat.NO_CONVERT)\n",
    "\n",
    "    # Initialize a variable to store the extracted Python code\n",
    "    python_code = None\n",
    "\n",
    "    # Iterate over the cells in the notebook\n",
    "    for cell in notebook.cells:\n",
    "        # Check if the cell type matches the specified type\n",
    "        if cell.cell_type == cell_type:\n",
    "            # Append the cell's source code to the python_code variable\n",
    "            if not python_code:\n",
    "                python_code = cell.source\n",
    "            else:\n",
    "                python_code += \"\\n\" + cell.source\n",
    "\n",
    "    # Return the extracted Python code\n",
    "    return python_code\n",
    "\n",
    "# Extracts the Python code from a .py file from the local filesystem\n",
    "def extract_python_code_from_py(local_file_path):\n",
    "    # Read the Python file from the local file system\n",
    "    with open(local_file_path, 'r', encoding='utf-8') as f:\n",
    "        python_code = f.read()\n",
    "\n",
    "    # Return the extracted Python code\n",
    "    return python_code\n",
    "\n",
    "# Read the list of file paths from the specified text file\n",
    "with open(FILE_URL_PATH + FILE_URL_NAME) as f:\n",
    "    code_files_paths = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the extracted code documents\n",
    "code_strings = []\n",
    "\n",
    "# Iterate over the list of file URLs\n",
    "for i in range(0, len(code_files_paths)):\n",
    "    # Check if the file URL ends with \".py\"\n",
    "    if code_files_paths[i].endswith(\".py\"):\n",
    "        # Extract the Python code from the .py file\n",
    "        content = extract_python_code_from_py(code_files_paths[i])\n",
    "        # Create a Document object with the extracted content and metadata\n",
    "        doc = Document(page_content=content, metadata={\"url\": code_files_paths[i], \"file_index\": i})\n",
    "        # Append the Document object to the code_strings list\n",
    "        code_strings.append(doc)\n",
    "        # Check if the file URL ends with \".py\"\n",
    "    elif code_files_paths[i].endswith(\".ipynb\"):\n",
    "        # Extract the Python code from the .py file\n",
    "        content_ipynb = extract_python_code_from_ipynb(code_files_paths[i])\n",
    "        # Create a Document object with the extracted content and metadata\n",
    "        doc_ipynb = Document(page_content=content_ipynb, metadata={\"url\": code_files_paths[i], \"file_index\": i})\n",
    "        # Append the Document object to the code_strings list\n",
    "        code_strings.append(doc_ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a text splitter for chunking the code strings\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,  # Specify the language as Python\n",
    "    chunk_size=20000,           # Set the chunk size to 1500 characters\n",
    "    chunk_overlap=2000          # Set the chunk overlap to 150 characters\n",
    ")\n",
    "\n",
    "# Split the code documents into chunks using the text splitter\n",
    "texts = text_splitter.split_documents(code_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of queries per minute (QPM) for embedding requests\n",
    "EMBEDDING_QPM = 100\n",
    "\n",
    "# Set the number of batches for processing embeddings\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "\n",
    "# Initialize an instance of the OpenAIEmbeddings class\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"  # Specify the model to use for generating embeddings\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index from the embedded code chunks\n",
    "# Use FAISS (Facebook AI Similarity Search) to create a searchable index\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the retriever from the FAISS index\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",      # Specify the search type as \"similarity\"\n",
    "    search_kwargs={\"k\": 5},        # Set search parameters, in this case, return the top 5 results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_question = \"Create a heatmap plot of the localisation status vs the UTR length\"\n",
    "#user_question = input(\"What would you like to ask the LLM?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"../../../../../../mariak/anndata_obj/sub_buckets/bcells_final.h5ad\",)\n",
    "cells_df = pd.DataFrame(adata.obs)\n",
    "cells_df.name = 'cells'\n",
    "\n",
    "genes_df = pd.DataFrame(adata.var)\n",
    "genes_df.name = 'genes'\n",
    "genes_df['gene_id'] = genes_df.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = lp.create_pandas_dataframe_agent(\n",
    "    dataframe_llm, [cells_df,genes_df], verbose=True, handle_parse_errors=True, allow_dangerous_code=True\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Based on the question asked and the dataframes provided, please perform the following steps:\n",
    "\n",
    "1. Identify the data asked for in the question.\n",
    "2. Based on step 1, find the relevant column names in the dataframes provided based on the information identified earlier in the question asked regarding data.\n",
    "3. The relevant column names can be a combination from the two dataframes.\n",
    "4. Provide the relevant column names from step 2 in a list.\n",
    "5. If the question asks for a gene name, do provide a gene name in addition to the relevant columns names.\n",
    "\"\"\"\n",
    "\n",
    "full_prompt = prompt + \"\\nQuestion: \" + user_question\n",
    "\n",
    "# the agent might raise an error. Sometimes repeating the same prompt helps...\n",
    "final_answer = agent.invoke(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_RAG = \"\"\" \n",
    "\n",
    "Context: {context}]\n",
    "\n",
    "The collection of Python scripts provided in the context, is designed to generate various types of data visualizations \n",
    "using the mdvtools library. Each script focuses on a specific type of plot and follows a common structure that includes loading \n",
    "data from a CSV file, creating a plot using specific parameters, and serving the visualization through an MDV project. \n",
    "\n",
    "All scripts in the context share a common workflow:\n",
    "\n",
    "Setup: Define the project path, data path, and view name, the project path should always be: project_path = os.path.expanduser('~/mdv/project')\n",
    "Plot function definition: Define the respective plot (dot plot, heatmap, histogram, box plot, scatter plot, 3D scatter plot, pie/ring chart, stacked row plot) using a function in the same way as the context.\n",
    "Project Creation: Initialize an MDVProject instance using the method: MDVProject(project_path, delete_existing=True).\n",
    "Data Loading: Load data from the specified CSV file into a pandas DataFrame using the load_data(path) function.\n",
    "Data adding: Add the data source to the project using the method: project.add_datasource(data_path, data).\n",
    "Plot Creation: Create the respective plot (dot plot, heatmap, histogram, box plot, scatter plot, 3D scatter plot, pie/ring chart, stacked row plot) and define the plot paramaters in the same way as in the context.\n",
    "Data Conversion: Convert the plot data to JSON format for integration with the MDV project using the convert_plot_to_json(plot) function.\n",
    "Serving: Configure the project view, set it to editable, and serve the project using the .set_view(view_name, plot_view), .set_editable(True) and .serve() methods.\n",
    "\n",
    "You are a top-class Python developer. Based on the question: {question}, decide which script from the context {context} is more relevant to the question: {question} and update the script to address the question.\n",
    "If no script is relevant, guided by the context generate a new script. \n",
    "This list \"\"\" + final_answer['output'] + \"\"\" specifies the names of the data fields that need to be plotted, for example in the params field. Get the structure of params definition from the context.\n",
    "The data should be loaded in the same way as in this notebook, in this case the lines of code to be used are below: \n",
    "import scanpy as sc\n",
    "adata = sc.read_h5ad(\"../../../../../../mariak/anndata_obj/sub_buckets/bcells_final.h5ad\",)\n",
    "cells_df = pd.DataFrame(adata.obs)\n",
    "cells_df.name = 'cells' \n",
    "\n",
    "If the prompt asks for a gene, make sure you load this datasource and that you create a link between the two datasets.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#The plot you should create is the same as the plot created in the context. Specify the parameters according to the respective files in the context for each plot type. DO NOT add any parameters that have not been defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PromptTemplate object using the defined RAG prompt\n",
    "prompt_RAG_template = PromptTemplate(\n",
    "    template=prompt_RAG,          # Specify the template string\n",
    "    input_variables=[\"context\", \"question\"]  # Define the input variables for the template\n",
    ")\n",
    "\n",
    "# Initialize a RetrievalQA chain using the specified language model, prompt template, and retriever\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm=code_llm,                 # Specify the language model to use\n",
    "    prompt=prompt_RAG_template,   # Use the defined prompt template\n",
    "    retriever=retriever,          # Use the initialized retriever for context retrieval\n",
    "    return_source_documents=True  # Configure the chain to return source documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context for the question (this should be retrieved by the retriever, but showing as an example)\n",
    "context = retriever\n",
    "\n",
    "# Invoke the QA chain with the query and context\n",
    "output = qa_chain.invoke({\"context\": context, \"query\": user_question})\n",
    "result = output[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pp(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the file urls retrieved from the context \n",
    "\n",
    "context_information = output['source_documents']\n",
    "context_information_metadata = [context_information[i].metadata for i in range(len(context_information))]\n",
    "context_information_metadata_url = [context_information_metadata[i]['url'] for i in range(len(context_information_metadata))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_from_response(response):\n",
    "    \"\"\"Extracts Python code from a string response.\"\"\"\n",
    "    # Use a regex pattern to match content between triple backticks\n",
    "    code_pattern = r\"```python(.*?)```\"\n",
    "    match = re.search(code_pattern, response, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        # Extract the matched code and strip any leading/trailing whitespaces\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "code = extract_code_from_response(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_script = code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_parameters(script, dataframe_path):\n",
    "    # Load the dataframe to infer the column types\n",
    "    df = pd.read_csv(dataframe_path)\n",
    "    #df['leiden'] = df['leiden'].apply(str)\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "    def is_categorical(column):\n",
    "        return column in categorical_columns\n",
    "\n",
    "    def is_numerical(column):\n",
    "        return column in numerical_columns\n",
    "    \n",
    "    # Define a regex pattern to find function definitions that create BoxPlots\n",
    "    patterns = [re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)BoxPlot\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)DotPlot\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)AbundanceBoxPlot\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)ViolinPlot\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)RingChart\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)RowChart\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)StackedRowChart\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)HeatmapPlot\\((.*?)\\)', re.DOTALL)]\n",
    "    \n",
    "    pattern_multiline = re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)MultiLinePlot\\((.*?)\\)', re.DOTALL)\n",
    "\n",
    "    for pattern in patterns:\n",
    "        if pattern.search(script):\n",
    "            \n",
    "            # Define a regex pattern to find params and param patterns\n",
    "            pattern_param = re.compile(r'params\\s*=\\s*\\[.*?\\]|param\\s*=\\s*\".*?\"')\n",
    "            \n",
    "            def reorder_params(match_param):\n",
    "                matched_text = match_param.group(0)  # Get the entire matched text\n",
    "\n",
    "                # Extract parameter names\n",
    "                if 'params' in matched_text:\n",
    "                    param_list = re.findall(r'\\'(.*?)\\'', matched_text)\n",
    "                    param_list = re.findall(r'\\\"(.*?)\\\"', matched_text)\n",
    "                else:\n",
    "                    param_list = [re.findall(r'\\\"(.*?)\\\"', matched_text)[0]]\n",
    "                \n",
    "                # Check for the presence of categorical and numerical variables\n",
    "                has_categorical = any(is_categorical(param) for param in param_list)\n",
    "                has_numerical = any(is_numerical(param) for param in param_list)\n",
    "\n",
    "                # Add a categorical variable if none is present\n",
    "                if not has_categorical and categorical_columns:\n",
    "                    param_list.insert(0, categorical_columns[0])\n",
    "                    has_categorical = True\n",
    "\n",
    "                if len(param_list) < 2:\n",
    "                    return matched_text  # No need to reorder if there are fewer than 2 parameters\n",
    "\n",
    "                first_param = param_list[0]\n",
    "                second_param = param_list[1]\n",
    "\n",
    "                # Check the types of the parameters using the dataframe\n",
    "                #if first_param in df.columns and second_param in df.columns:\n",
    "                if has_categorical and has_numerical:\n",
    "                    if not (is_categorical(first_param) and is_numerical(second_param)):\n",
    "                        param_list[0], param_list[1] = param_list[1], param_list[0]\n",
    "\n",
    "                # Reconstruct the parameters with reordered values\n",
    "                if 'params' in matched_text:\n",
    "                    reordered_params = f\"params = ['{param_list[0]}', '{param_list[1:]}']\"\n",
    "                else:\n",
    "                    reordered_params = f'param = \"{param_list[0]}\"'\n",
    "\n",
    "                return reordered_params.replace('\\'[', ' ').replace(']\\'','')\n",
    "\n",
    "            # Substitute the matches with reordered parameters\n",
    "            modified_script = re.sub(pattern_param, reorder_params, script)\n",
    "\n",
    "            return modified_script\n",
    "\n",
    "    if pattern_multiline.search(script):\n",
    "        # Define a regex pattern to find params and param patterns\n",
    "        pattern_param = re.compile(r'params\\s*=\\s*\\[.*?\\]|param\\s*=\\s*\".*?\"')\n",
    "        \n",
    "        def reorder_params_multiline(match_param):\n",
    "            matched_text = match_param.group(0)  # Get the entire matched text\n",
    "\n",
    "            # Extract parameter names\n",
    "            if 'params' in matched_text:\n",
    "                param_list = re.findall(r'\\'(.*?)\\'', matched_text)\n",
    "                param_list = re.findall(r'\\\"(.*?)\\\"', matched_text)\n",
    "            else:\n",
    "                param_list = [re.findall(r'\\\"(.*?)\\\"', matched_text)[0]]\n",
    "            \n",
    "            # Check for the presence of categorical and numerical variables\n",
    "            has_categorical = any(is_categorical(param) for param in param_list)\n",
    "            has_numerical = any(is_numerical(param) for param in param_list)\n",
    "\n",
    "            # Add a categorical variable if none is present\n",
    "            if not has_categorical and categorical_columns:\n",
    "                param_list.insert(0, categorical_columns[0])\n",
    "                has_categorical = True\n",
    "\n",
    "            if len(param_list) < 2:\n",
    "                return matched_text  # No need to reorder if there are fewer than 2 parameters\n",
    "\n",
    "            first_param = param_list[0]\n",
    "            second_param = param_list[1]\n",
    "\n",
    "            # Check the types of the parameters using the dataframe\n",
    "            #if first_param in df.columns and second_param in df.columns:\n",
    "            if has_categorical and has_numerical:\n",
    "                if not (is_numerical(first_param) and is_categorical(second_param)):\n",
    "                    param_list[0], param_list[1] = param_list[1], param_list[0]\n",
    "\n",
    "            # Reconstruct the parameters with reordered values\n",
    "            if 'params' in matched_text:\n",
    "                reordered_params = f\"params = ['{param_list[0]}', '{param_list[1:]}']\"\n",
    "            else:\n",
    "                reordered_params = f'param = \"{param_list[0]}\"'\n",
    "\n",
    "            return reordered_params.replace('\\'[', ' ').replace(']\\'','')\n",
    "\n",
    "        # Substitute the matches with reordered parameters\n",
    "        modified_script_multiline = re.sub(pattern_param, reorder_params_multiline, script)\n",
    "\n",
    "        return modified_script_multiline\n",
    "\n",
    "        \n",
    "    return script\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the reorder transformation\n",
    "modified_script = original_script#reorder_parameters(original_script, path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_functions = \"\"\"import os\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import sys\n",
    "import  numpy as np\n",
    "from mdvtools.mdvproject import MDVProject\n",
    "from mdvtools.charts.heatmap_plot import HeatmapPlot\n",
    "from mdvtools.charts.histogram_plot import HistogramPlot\n",
    "from mdvtools.charts.dot_plot import DotPlot\n",
    "from mdvtools.charts.box_plot import BoxPlot\n",
    "from mdvtools.charts.scatter_plot_3D import ScatterPlot3D\n",
    "from mdvtools.charts.row_chart import RowChart\n",
    "from mdvtools.charts.scatter_plot import ScatterPlot\n",
    "from mdvtools.charts.abundance_box_plot import AbundanceBoxPlot\n",
    "from mdvtools.charts.stacked_row_plot import StackedRowChart\n",
    "from mdvtools.charts.ring_chart import RingChart\n",
    "from mdvtools.charts.violin_plot import ViolinPlot\n",
    "from mdvtools.charts.multi_line_plot import MultiLinePlot\n",
    "from mdvtools.charts.pie_chart import PieChart\n",
    "\n",
    "import json \\n\n",
    "\\n\n",
    "\n",
    "def load_data(path):\n",
    "    #Load data from the specified CSV file.\n",
    "    return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "def convert_plot_to_json(plot):\n",
    "    #Convert plot data to JSON format.\n",
    "    return json.loads(json.dumps(plot.plot_data, indent=2).replace(\"\\\\\\\\\", \"\"))\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into lines\n",
    "lines = modified_script.splitlines()\n",
    "\n",
    "# # Find the starting line index\n",
    "start_index = next((i for i, line in enumerate(lines) if not line.strip().startswith(('import', 'from'))), None)\n",
    "\n",
    "if start_index is not None:\n",
    "    # Capture all lines starting from the first 'def'\n",
    "    captured_lines = \"\\n\".join(lines[start_index:])\n",
    "    #print(\"Captured part:\\n\", captured_lines)\n",
    "else:\n",
    "    print(\"Pattern not found\")\n",
    "\n",
    "with open(\"temp_code_3.py\", \"w\") as f:\n",
    "    f.write(packages_functions)\n",
    "    f.write(captured_lines)\n",
    "    #f.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(captured_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "log_to_json(context_information_metadata_url, output['query'], prompt_RAG, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the saved Python file. This will start a server on localhost:5050, open the browser and display the plot with the server continuing to run in the background.\n",
    "%run temp_code_3.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvjupyter",
   "language": "python",
   "name": "venvjupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
