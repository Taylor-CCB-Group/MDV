{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Generation using Retrieval Augmented Generation + LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "\n",
    "import nbformat\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import regex as re\n",
    "import datetime\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import langchain_experimental.agents.agent_toolkits.pandas.base as lp\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"Can you please generate a chart of the leiden clusters?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting keys and variables\n",
    "# .env file should have OPENAI_API_KEY & GITHUB_TOKEN\n",
    "# also currently need a key.json file with google-sheet config\n",
    "load_dotenv()\n",
    "# OPENAI_API_KEY environment variable will be used internally by OpenAI modules\n",
    "GITHUB_TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "\n",
    "GITHUB_REPO = \"Taylor-CCB-Group/MDV\" # @param {type:\"string\"}\n",
    "BRANCH_NAME = \"mk-API\"\n",
    "PROJECT_PATH_1 = \"python/mdvtools/charts\"\n",
    "PROJECT_PATH_2 = \"python/mdvtools/test_projects\"\n",
    "\n",
    "FILE_URL_PATH = \"./\"\n",
    "FILE_URL_NAME = \"code_files_URL.txt\"\n",
    "\n",
    "mypath = os.path.abspath('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the Google Sheets connection\n",
    "def init_google_sheet(json_keyfile_path, sheet_name):\n",
    "    scopes = [\"https://www.googleapis.com/auth/spreadsheets\",\n",
    "             \"https://www.googleapis.com/auth/drive\"]\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(json_keyfile_path, scopes)\n",
    "    gc = gspread.authorize(credentials)\n",
    "    sheet = gc.open(sheet_name).sheet1  # Opens the first sheet in the spreadsheet\n",
    "    return sheet\n",
    "\n",
    "# Function to log data to the Google Sheet\n",
    "def log_to_google_sheet(sheet, context, prompt, prompt_template, response):\n",
    "    timestamp = datetime.datetime.now(datetime.UTC).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    data = [timestamp, context, prompt, prompt_template, response]\n",
    "    sheet.append_row(data)  # Appends a row with the prompt and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Google Sheets connection\n",
    "json_keyfile_path = os.path.join(mypath, \"key.json\")\n",
    "sheet_name = 'Evaluation_LLM'  # TODO: Update this with your sheet's name\n",
    "sheet = init_google_sheet(json_keyfile_path, sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an instance of the ChatOpenAI class with specified parameters\n",
    "# Set the temperature to 0.1 for more deterministic responses\n",
    "# Specify the model to use as \"gpt-4o\"\n",
    "\n",
    "code_llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4o\")\n",
    "\n",
    "dataframe_llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_github_repo(url, is_sub_dir, branch_name, project_path, access_token=f\"{GITHUB_TOKEN}\"):\n",
    "    \"\"\"\n",
    "    Crawls a GitHub repository to retrieve file URLs based on specified criteria.\n",
    "\n",
    "    Args:\n",
    "        url (str): The GitHub repository URL or sub-directory URL.\n",
    "        is_sub_dir (bool): Flag indicating if the current URL is a sub-directory.\n",
    "        branch_name (str): The branch name to crawl.\n",
    "        project_path (str): The path of the project in the repository.\n",
    "        access_token (str, optional): GitHub access token for authentication. Defaults to GITHUB_TOKEN.\n",
    "\n",
    "    Returns:\n",
    "        list: List of file URLs that match the criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of files to ignore\n",
    "    ignore_list = ['__init__.py', 'pbmc3k_tutorial.ipynb', 'pbmc3k_tutorial.py']\n",
    "\n",
    "    # Determine the appropriate API URL based on whether it's a sub-directory\n",
    "    if not is_sub_dir:\n",
    "        api_url = f\"https://api.github.com/repos/{url}/contents/{project_path}?ref={branch_name}\"\n",
    "    else:\n",
    "        api_url = url\n",
    "\n",
    "    # Set up headers for the GitHub API request, including authorization\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github.v3+json\",\n",
    "        \"Authorization\": f\"Bearer {access_token}\"\n",
    "    }\n",
    "\n",
    "    # Make a GET request to the GitHub API\n",
    "    response = requests.get(api_url, headers=headers)\n",
    "    # Raise an exception for any request errors\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Initialize an empty list to store file URLs\n",
    "    files = []\n",
    "\n",
    "    # Parse the JSON response content\n",
    "    contents = response.json()\n",
    "\n",
    "    # Iterate over the items in the contents\n",
    "    for item in contents:\n",
    "        # Check if the item is a file and meets the criteria for inclusion\n",
    "        if item['type'] == 'file' and item['name'] not in ignore_list and (item['name'].endswith('.py') or item['name'].endswith('.ipynb')):\n",
    "            files.append(item['html_url'])\n",
    "        # Check if the item is a directory (excluding hidden ones)\n",
    "        elif item['type'] == 'dir' and not item['name'].startswith(\".\"):\n",
    "            # Recursively crawl the sub-directory\n",
    "            sub_files = crawl_github_repo(item['url'], True, branch_name, project_path)\n",
    "            # Pause briefly to avoid rate limiting\n",
    "            time.sleep(.1)\n",
    "            # Add the sub-directory files to the list\n",
    "            files.extend(sub_files)\n",
    "\n",
    "    # Return the list of collected file URLs\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl the GitHub repository to get a list of relevant file URLs\n",
    "#code_files_urls_1 = crawl_github_repo(GITHUB_REPO, False, BRANCH_NAME, PROJECT_PATH_1, GITHUB_TOKEN)\n",
    "code_files_urls_2 = crawl_github_repo(GITHUB_REPO, False, BRANCH_NAME, PROJECT_PATH_2, GITHUB_TOKEN)\n",
    "#code_files_urls = code_files_urls_1 + code_files_urls_2\n",
    "code_files_urls = code_files_urls_2\n",
    "\n",
    "# Write the list of file URLs to a specified text file\n",
    "with open(FILE_URL_PATH + FILE_URL_NAME, 'w') as f:\n",
    "    # Iterate through the list of file URLs and write each one to the file\n",
    "    for item in code_files_urls:\n",
    "        f.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the Python code from a .ipynb (Jupyter Notebook) file from GitHub\n",
    "def extract_python_code_from_ipynb(github_url, cell_type=\"code\"):\n",
    "    # Convert the GitHub URL to the raw content URL\n",
    "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "\n",
    "    # Make a GET request to fetch the raw content of the notebook\n",
    "    response = requests.get(raw_url)\n",
    "    response.raise_for_status()  # Check for any request errors\n",
    "\n",
    "    # Get the content of the notebook as text\n",
    "    notebook_content = response.text\n",
    "\n",
    "    # Read the notebook content using nbformat\n",
    "    notebook = nbformat.reads(notebook_content, as_version=nbformat.NO_CONVERT)\n",
    "\n",
    "    # Initialize a variable to store the extracted Python code\n",
    "    python_code = None\n",
    "\n",
    "    # Iterate over the cells in the notebook\n",
    "    for cell in notebook.cells:\n",
    "        # Check if the cell type matches the specified type\n",
    "        if cell.cell_type == cell_type:\n",
    "            # Append the cell's source code to the python_code variable\n",
    "            if not python_code:\n",
    "                python_code = cell.source\n",
    "            else:\n",
    "                python_code += \"\\n\" + cell.source\n",
    "\n",
    "    # Return the extracted Python code\n",
    "    return python_code\n",
    "\n",
    "# Extracts the Python code from a .py file from GitHub\n",
    "def extract_python_code_from_py(github_url):\n",
    "    # Convert the GitHub URL to the raw content URL\n",
    "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "    \n",
    "    # Make a GET request to fetch the raw content of the Python file\n",
    "    response = requests.get(raw_url)\n",
    "    response.raise_for_status()  # Check for any request errors\n",
    "\n",
    "    # Get the content of the Python file as text\n",
    "    python_code = response.text\n",
    "\n",
    "    # Return the extracted Python code\n",
    "    return python_code\n",
    "\n",
    "# Read the list of file URLs from the specified text file\n",
    "with open(FILE_URL_PATH + FILE_URL_NAME) as f:\n",
    "    code_files_urls = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the extracted code documents\n",
    "code_strings = []\n",
    "\n",
    "# Iterate over the list of file URLs\n",
    "for i in range(0, len(code_files_urls)):\n",
    "    # Check if the file URL ends with \".py\"\n",
    "    if code_files_urls[i].endswith(\".py\"):\n",
    "        # Extract the Python code from the .py file\n",
    "        content = extract_python_code_from_py(code_files_urls[i])\n",
    "        # Create a Document object with the extracted content and metadata\n",
    "        doc = Document(page_content=content, metadata={\"url\": code_files_urls[i], \"file_index\": i})\n",
    "        # Append the Document object to the code_strings list\n",
    "        code_strings.append(doc)\n",
    "        # Check if the file URL ends with \".py\"\n",
    "    elif code_files_urls[i].endswith(\".ipynb\"):\n",
    "        # Extract the Python code from the .py file\n",
    "        content_ipynb = extract_python_code_from_ipynb(code_files_urls[i])\n",
    "        # Create a Document object with the extracted content and metadata\n",
    "        doc_ipynb = Document(page_content=content_ipynb, metadata={\"url\": code_files_urls[i], \"file_index\": i})\n",
    "        # Append the Document object to the code_strings list\n",
    "        code_strings.append(doc_ipynb)\n",
    "\n",
    "# Uncomment to display the first document in the code_strings list\n",
    "#code_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a text splitter for chunking the code strings\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,  # Specify the language as Python\n",
    "    chunk_size=20000,           # Set the chunk size to 1500 characters\n",
    "    chunk_overlap=2000          # Set the chunk overlap to 150 characters\n",
    ")\n",
    "\n",
    "# Split the code documents into chunks using the text splitter\n",
    "texts = text_splitter.split_documents(code_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of queries per minute (QPM) for embedding requests\n",
    "EMBEDDING_QPM = 100\n",
    "\n",
    "# Set the number of batches for processing embeddings\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "\n",
    "# Initialize an instance of the OpenAIEmbeddings class\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"  # Specify the model to use for generating embeddings\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index from the embedded code chunks\n",
    "# Use FAISS (Facebook AI Similarity Search) to create a searchable index\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the retriever from the FAISS index\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",      # Specify the search type as \"similarity\"\n",
    "    search_kwargs={\"k\": 5},        # Set search parameters, in this case, return the top 5 results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_question = \"Create a heatmap plot of the localisation status vs the UTR length\"\n",
    "#user_question = input(\"What would you like to ask the LLM?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = os.path.join(mypath, \"sample_data/data_cells.csv\")\n",
    "df = pd.read_csv(path_to_data)\n",
    "df['leiden'] = df['leiden'].apply(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df#.dropna().iloc[:2,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = lp.create_pandas_dataframe_agent(\n",
    "    dataframe_llm, df_short, verbose=True, handle_parse_errors=True, allow_dangerous_code=True\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Based on the question asked and the CSV, please perform the following steps:\n",
    "\n",
    "1. Identify the data asked for in the question.\n",
    "2. Based on step 1, find the relevant column names in the CSV based on the information identified earlier in the question asked regarding data.\n",
    "3. Provide the relevant column names from step 2 in a list.\n",
    "\"\"\"\n",
    "\n",
    "full_prompt = prompt + \"\\nQuestion: \" + user_question\n",
    "\n",
    "# the agent might raise an error. Sometimes repeating the same prompt helps...\n",
    "final_answer = agent.invoke(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_RAG = \"\"\" You can only generate python code based on the provided context. If a response cannot be formed strictly using the context, politely say you need more information to generate the plot.\n",
    "\n",
    "Context: {context}]\n",
    "\n",
    "The collection of Python scripts provided in the context, is designed to generate various types of data visualizations \n",
    "using the mdvtools library. Each script focuses on a specific type of plot and follows a common structure that includes loading \n",
    "data from a CSV file, creating a plot using specific parameters, and serving the visualization through an MDV project. \n",
    "\n",
    "All scripts in the context share a common workflow:\n",
    "\n",
    "Setup: Define the project path, data path, and view name, the project path should always be: project_path = os.path.expanduser('~/mdv/project')\n",
    "Plot function definition: Define the respective plot (dot plot, heatmap, histogram, box plot, scatter plot, 3D scatter plot, pie/ring chart, stacked row plot) using a function in the same way as the context.\n",
    "Project Creation: Initialize an MDVProject instance using the method: MDVProject(project_path, delete_existing=True).\n",
    "Data Loading: Load data from the specified CSV file into a pandas DataFrame using the load_data(path) function.\n",
    "Data adding: Add the data source to the project using the method: project.add_datasource(data_path, data).\n",
    "Plot Creation: Create the respective plot (dot plot, heatmap, histogram, box plot, scatter plot, 3D scatter plot, pie/ring chart, stacked row plot) and define the plot paramaters in the same way as in the context.\n",
    "Data Conversion: Convert the plot data to JSON format for integration with the MDV project using the convert_plot_to_json(plot) function.\n",
    "Serving: Configure the project view, set it to editable, and serve the project using the .set_view(view_name, plot_view), .set_editable(True) and .serve() methods.\n",
    "\n",
    "You are a top-class Python developer. Generate a Python script following the workflow detailed above and use exactly the same lines of code as the scripts in the context. \n",
    "The data should be loaded from a csv provided, the path to the csv is given by: \"\"\" + path_to_data + \"\"\" \n",
    "This list \"\"\" + final_answer['output'] + \"\"\" specifies the names of the data fields that need to be plotted, for example in the params field. Get the structure of params definition from the context. \n",
    "The question: {question} specifies the plot required. \n",
    "\"\"\"\n",
    "\n",
    "#The plot you should create is the same as the plot created in the context. Specify the parameters according to the respective files in the context for each plot type. DO NOT add any parameters that have not been defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PromptTemplate object using the defined RAG prompt\n",
    "prompt_RAG_template = PromptTemplate(\n",
    "    template=prompt_RAG,          # Specify the template string\n",
    "    input_variables=[\"context\", \"question\"]  # Define the input variables for the template\n",
    ")\n",
    "\n",
    "# Initialize a RetrievalQA chain using the specified language model, prompt template, and retriever\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm=code_llm,                 # Specify the language model to use\n",
    "    prompt=prompt_RAG_template,   # Use the defined prompt template\n",
    "    retriever=retriever,          # Use the initialized retriever for context retrieval\n",
    "    return_source_documents=True  # Configure the chain to return source documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context for the question (this should be retrieved by the retriever, but showing as an example)\n",
    "context = retriever\n",
    "\n",
    "# Invoke the QA chain with the query and context\n",
    "output = qa_chain.invoke({\"context\": context, \"query\": user_question})\n",
    "result = output[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the file urls retrieved from the context \n",
    "\n",
    "context_information = output['source_documents']\n",
    "context_information_metadata = [context_information[i].metadata for i in range(len(context_information))]\n",
    "context_information_metadata_url = [context_information_metadata[i]['url'] for i in range(len(context_information_metadata))]\n",
    "context_information_metadata_name = [s[82:] for s in context_information_metadata_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_from_response(response):\n",
    "    \"\"\"Extracts Python code from a string response.\"\"\"\n",
    "    # Use a regex pattern to match content between triple backticks\n",
    "    code_pattern = r\"```python(.*?)```\"\n",
    "    match = re.search(code_pattern, response, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        # Extract the matched code and strip any leading/trailing whitespaces\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "code = extract_code_from_response(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_script = code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_parameters(script, dataframe_path):\n",
    "    # Load the dataframe to infer the column types\n",
    "    df = pd.read_csv(dataframe_path)\n",
    "    #df['leiden'] = df['leiden'].apply(str)\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "    def is_categorical(column):\n",
    "        return column in categorical_columns\n",
    "\n",
    "    def is_numerical(column):\n",
    "        return column in numerical_columns\n",
    "    \n",
    "    # Define a regex pattern to find function definitions that create BoxPlots\n",
    "    patterns = [re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)BoxPlot\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)DotPlot\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)AbundanceBoxPlot\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)ViolinPlot\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)RingChart\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)RowChart\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)StackedRowChart\\((.*?)\\)', re.DOTALL),\n",
    "                re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)HeatmapPlot\\((.*?)\\)', re.DOTALL)]\n",
    "    \n",
    "    pattern_multiline = re.compile(r'def\\s+(\\w*)\\s*\\((.*?)\\):\\s*\\n(.*?)MultiLinePlot\\((.*?)\\)', re.DOTALL)\n",
    "\n",
    "    for pattern in patterns:\n",
    "        if pattern.search(script):\n",
    "            \n",
    "            # Define a regex pattern to find params and param patterns\n",
    "            pattern_param = re.compile(r'params\\s*=\\s*\\[.*?\\]|param\\s*=\\s*\".*?\"')\n",
    "            \n",
    "            def reorder_params(match_param):\n",
    "                matched_text = match_param.group(0)  # Get the entire matched text\n",
    "\n",
    "                # Extract parameter names\n",
    "                if 'params' in matched_text:\n",
    "                    param_list = re.findall(r'\\'(.*?)\\'', matched_text)\n",
    "                    param_list = re.findall(r'\\\"(.*?)\\\"', matched_text)\n",
    "                else:\n",
    "                    param_list = [re.findall(r'\\\"(.*?)\\\"', matched_text)[0]]\n",
    "                \n",
    "                # Check for the presence of categorical and numerical variables\n",
    "                has_categorical = any(is_categorical(param) for param in param_list)\n",
    "                has_numerical = any(is_numerical(param) for param in param_list)\n",
    "\n",
    "                # Add a categorical variable if none is present\n",
    "                if not has_categorical and categorical_columns:\n",
    "                    param_list.insert(0, categorical_columns[0])\n",
    "                    has_categorical = True\n",
    "\n",
    "                if len(param_list) < 2:\n",
    "                    return matched_text  # No need to reorder if there are fewer than 2 parameters\n",
    "\n",
    "                first_param = param_list[0]\n",
    "                second_param = param_list[1]\n",
    "\n",
    "                # Check the types of the parameters using the dataframe\n",
    "                #if first_param in df.columns and second_param in df.columns:\n",
    "                if has_categorical and has_numerical:\n",
    "                    if not (is_categorical(first_param) and is_numerical(second_param)):\n",
    "                        param_list[0], param_list[1] = param_list[1], param_list[0]\n",
    "\n",
    "                # Reconstruct the parameters with reordered values\n",
    "                if 'params' in matched_text:\n",
    "                    reordered_params = f\"params = ['{param_list[0]}', '{param_list[1:]}']\"\n",
    "                else:\n",
    "                    reordered_params = f'param = \"{param_list[0]}\"'\n",
    "\n",
    "                return reordered_params.replace('\\'[', ' ').replace(']\\'','')\n",
    "\n",
    "            # Substitute the matches with reordered parameters\n",
    "            modified_script = re.sub(pattern_param, reorder_params, script)\n",
    "\n",
    "            return modified_script\n",
    "\n",
    "    if pattern_multiline.search(script):\n",
    "        # Define a regex pattern to find params and param patterns\n",
    "        pattern_param = re.compile(r'params\\s*=\\s*\\[.*?\\]|param\\s*=\\s*\".*?\"')\n",
    "        \n",
    "        def reorder_params_multiline(match_param):\n",
    "            matched_text = match_param.group(0)  # Get the entire matched text\n",
    "\n",
    "            # Extract parameter names\n",
    "            if 'params' in matched_text:\n",
    "                param_list = re.findall(r'\\'(.*?)\\'', matched_text)\n",
    "                param_list = re.findall(r'\\\"(.*?)\\\"', matched_text)\n",
    "            else:\n",
    "                param_list = [re.findall(r'\\\"(.*?)\\\"', matched_text)[0]]\n",
    "            \n",
    "            # Check for the presence of categorical and numerical variables\n",
    "            has_categorical = any(is_categorical(param) for param in param_list)\n",
    "            has_numerical = any(is_numerical(param) for param in param_list)\n",
    "\n",
    "            # Add a categorical variable if none is present\n",
    "            if not has_categorical and categorical_columns:\n",
    "                param_list.insert(0, categorical_columns[0])\n",
    "                has_categorical = True\n",
    "\n",
    "            if len(param_list) < 2:\n",
    "                return matched_text  # No need to reorder if there are fewer than 2 parameters\n",
    "\n",
    "            first_param = param_list[0]\n",
    "            second_param = param_list[1]\n",
    "\n",
    "            # Check the types of the parameters using the dataframe\n",
    "            #if first_param in df.columns and second_param in df.columns:\n",
    "            if has_categorical and has_numerical:\n",
    "                if not (is_numerical(first_param) and is_categorical(second_param)):\n",
    "                    param_list[0], param_list[1] = param_list[1], param_list[0]\n",
    "\n",
    "            # Reconstruct the parameters with reordered values\n",
    "            if 'params' in matched_text:\n",
    "                reordered_params = f\"params = ['{param_list[0]}', '{param_list[1:]}']\"\n",
    "            else:\n",
    "                reordered_params = f'param = \"{param_list[0]}\"'\n",
    "\n",
    "            return reordered_params.replace('\\'[', ' ').replace(']\\'','')\n",
    "\n",
    "        # Substitute the matches with reordered parameters\n",
    "        modified_script_multiline = re.sub(pattern_param, reorder_params_multiline, script)\n",
    "\n",
    "        return modified_script_multiline\n",
    "\n",
    "        \n",
    "    return script\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the reorder transformation\n",
    "modified_script = reorder_parameters(original_script, path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_functions = \"\"\"import os\n",
    "import pandas as pd\n",
    "from mdvtools.mdvproject import MDVProject\n",
    "from mdvtools.charts.heatmap_plot import HeatmapPlot\n",
    "from mdvtools.charts.histogram_plot import HistogramPlot\n",
    "from mdvtools.charts.dot_plot import DotPlot\n",
    "from mdvtools.charts.box_plot import BoxPlot\n",
    "from mdvtools.charts.scatter_plot_3D import ScatterPlot3D\n",
    "from mdvtools.charts.row_chart import RowChart\n",
    "from mdvtools.charts.scatter_plot import ScatterPlot\n",
    "from mdvtools.charts.abundance_box_plot import AbundanceBoxPlot\n",
    "from mdvtools.charts.stacked_row_plot import StackedRowChart\n",
    "from mdvtools.charts.ring_chart import RingChart\n",
    "from mdvtools.charts.violin_plot import ViolinPlot\n",
    "from mdvtools.charts.multi_line_plot import MultiLinePlot\n",
    "from mdvtools.charts.pie_chart import PieChart\n",
    "\n",
    "import json \\n\n",
    "\\n\n",
    "\n",
    "def load_data(path):\n",
    "    #Load data from the specified CSV file.\n",
    "    return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "def convert_plot_to_json(plot):\n",
    "    #Convert plot data to JSON format.\n",
    "    return json.loads(json.dumps(plot.plot_data, indent=2).replace(\"\\\\\\\\\", \"\"))\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into lines\n",
    "lines = modified_script.splitlines()\n",
    "\n",
    "# Find the starting line index\n",
    "start_index = next((i for i, line in enumerate(lines) if line.strip().startswith('def')), None)\n",
    "\n",
    "if start_index is not None:\n",
    "    # Capture all lines starting from the first 'def'\n",
    "    captured_lines = \"\\n\".join(lines[start_index:])\n",
    "    #print(\"Captured part:\\n\", captured_lines)\n",
    "else:\n",
    "    print(\"Pattern not found\")\n",
    "\n",
    "with open(\"temp_code_3.py\", \"w\") as f:\n",
    "    f.write(packages_functions)\n",
    "    f.write(captured_lines)\n",
    "    #f.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the prompt and the output of the LLM to the google sheets\n",
    "log_to_google_sheet(sheet, str(context_information_metadata_name), output['query'], prompt_RAG, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the saved Python file. This will start a server on localhost:5050, open the browser and display the plot with the server continuing to run in the background.\n",
    "%run temp_code_3.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvjup",
   "language": "python",
   "name": "venvjup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
